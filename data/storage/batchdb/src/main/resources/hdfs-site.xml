<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>dfs.namenode.logging.level</name>
    <value>info</value>
    <description>The logging level for dfs namenode. Other values are "dir"(trac
      e namespace mutations), "block"(trace block under/over replications and block
      creations/deletions), or "all".</description>
  </property>

  <property>
    <name>dfs.namenode.handler.count</name>
    <value>30</value>
    <description>The number of server threads for the namenode.</description>
  </property>

  <property>
    <name>dfs.datanode.handler.count</name>
    <value>5</value>
    <description>The number of server threads for the datanode.</description>
  </property>

  <property>
    <name>dfs.datanode.max.xcievers</name>
    <value>8192</value>
    <description>An upper bound on connections was added in Hadoop (HADOOP-3633/HADOOP-3859)</description>
  </property>


  <property>
    <name>dfs.replication</name>
    <value>3</value>
    <description>Default block replication. 
      The actual number of replications can be specified when the file is created.
      The default is used if replication is not specified in create time.
    </description>
  </property>

<!--
  <property> 
    <name>dfs.socket.timeout</name> 
    <value>300000</value> 
    <description>dfs socket timeout</description> 
  </property>
-->

  <property> 
    <name>dfs.datanode.socket.write.timeout</name> 
    <value>0</value> 
    <description>datanode write timeout</description> 
  </property>

  <property>
    <name>dfs.https.enable</name>
    <value>false</value>
    <description>Decide if HTTPS(SSL) is supported on HDFS
    </description>
  </property>

  <property>
    <name>dfs.https.need.client.auth</name>
    <value>false</value>
    <description>Whether SSL client certificate authentication is required
    </description>
  </property>

  <property>
    <name>dfs.name.dir</name>
    <value>${hadoop.dfs.dir}/name</value>
    <description>Determines where on the local filesystem the DFS name node
      should store the name table(fsimage).  If this is a comma-delimited list
      of directories then the name table is replicated in all of the
      directories, for redundancy. </description>
  </property>

  <property>
    <name>dfs.name.edits.dir</name>
    <value>${dfs.name.dir}</value>
    <description>Determines where on the local filesystem the DFS name node
      should store the transaction (edits) file. If this is a comma-delimited list
      of directories then the transaction file is replicated in all of the 
      directories, for redundancy. Default value is same as dfs.name.dir
    </description>
  </property>

  <property>
    <name>dfs.web.ugi</name>
    <value>moom,moom</value>
    <description>The user account used by the web interface.  Syntax: USERNAME,GROUP1,GROUP2, ...</description>
  </property>

  <property>
    <name>dfs.data.dir</name>
    <value>${hadoop.dfs.dir}/data</value>
    <description>Determines where on the local filesystem an DFS data node
      should store its blocks.  If this is a comma-delimited
      list of directories, then data will be stored in all named
      directories, typically on different devices.
      Directories that do not exist are ignored.
    </description>
  </property>
</configuration>
